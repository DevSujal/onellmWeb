<p align="center">
  <img src="https://raw.githubusercontent.com/onellm/onellm-java/main/assets/logo.png" alt="OneLLM Logo" width="180"/>
</p>

<h1 align="center">ğŸš€ OneLLM</h1>

<p align="center">
  <strong>One Interface. Twelve Providers. Your API Keys.</strong>
</p>

<p align="center">
  <a href="#-quick-start">Quick Start</a> â€¢
  <a href="#-providers">Providers</a> â€¢
  <a href="#-rest-api">REST API</a> â€¢
  <a href="#-sdk-usage">SDK Usage</a> â€¢
  <a href="#-configuration">Configuration</a>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Java-17+-blue?style=for-the-badge&logo=openjdk" alt="Java 17+"/>
  <img src="https://img.shields.io/badge/Spring%20Boot-3.2.0-brightgreen?style=for-the-badge&logo=springboot" alt="Spring Boot 3.2.0"/>
  <img src="https://img.shields.io/badge/License-MIT-yellow?style=for-the-badge" alt="MIT License"/>
  <img src="https://img.shields.io/badge/BYOK-Bring%20Your%20Own%20Key-orange?style=for-the-badge" alt="BYOK"/>
</p>

---

**OneLLM** is a unified Java SDK and REST API that provides a single interface for calling **10 different LLM providers**. Users bring their own API keys â€” just specify the model name and your API key, and OneLLM automatically routes your request to the right provider.

> ğŸ”‘ **Bring Your Own Key (BYOK)**: OneLLM doesn't store or require server-side API keys. Users provide their own API keys in each request, making it perfect for multi-tenant applications.

## âœ¨ Features

| Feature | Description |
|---------|-------------|
| ğŸ”Œ **12 Providers** | OpenAI, Anthropic, Google Gemini, Azure OpenAI, Groq, Cerebras, Ollama, OpenRouter, xAI, GitHub Copilot, Hugging Face, and **FreeLLM** (Free!) |
| ğŸ”‘ **BYOK Model** | Users provide their own API keys â€” no server-side credential storage |
| ğŸ¯ **Auto-Routing** | Automatically routes requests based on model name |
| ğŸŒŠ **Streaming** | Full support for streaming responses via SSE |
| âš¡ **Async** | Non-blocking async completions with `CompletableFuture` |
| ğŸ›¡ï¸ **Type-Safe** | Builder pattern with validation for all request parameters |
| ğŸ“Š **Usage Tracking** | Token usage and latency metrics in every response |

---

## ğŸš€ Quick Start

### Prerequisites

- Java 17 or higher
- Maven 3.6+

### Run the Server

```bash
# Clone and build
git clone https://github.com/onellm/onellm-java.git
cd onellm

# Run (no API keys needed - users provide their own!)
mvn spring-boot:run
```

The server starts at `http://localhost:8080`

### Make Your First Request

```bash
curl -X POST http://localhost:8080/api/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "apiKey": "YOUR_OPENAI_API_KEY",
    "model": "gpt-4",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'
```

> **Note**: Replace `YOUR_OPENAI_API_KEY` with your actual API key from the respective provider.

---

## ğŸ”Œ Providers

OneLLM supports **10 LLM providers** out of the box:

| Provider | Models | Required Fields |
|----------|--------|-----------------|
| **OpenAI** | `gpt-4`, `gpt-4-turbo`, `gpt-4o`, `gpt-3.5-turbo`, `o1`, `o3`, `chatgpt-*` | `apiKey` |
| **Anthropic** | `claude-3-opus`, `claude-3-sonnet`, `claude-3-haiku`, `claude-3.5-sonnet`, `claude-4-*` | `apiKey` |
| **Google Gemini** | `gemini-pro`, `gemini-ultra`, `gemini-1.5-pro`, `gemini-2.0-flash` | `apiKey` |
| **Azure OpenAI** | Your deployed models | `apiKey`, `azureResourceName`, `azureDeploymentName` |
| **Groq** | `llama-3`, `mixtral`, `gemma` | `apiKey` |
| **Cerebras** | `cerebras-gpt` variants | `apiKey` |
| **Ollama** | Any local model | `baseUrl` (optional, defaults to localhost) |
| **OpenRouter** | 100+ models | `apiKey`, optionally `openRouterSiteName`, `openRouterSiteUrl` |
| **xAI** | `grok-*` models | `apiKey` |
| **GitHub Copilot** | Copilot models | `apiKey` |
| **Hugging Face** | `meta-llama/*`, `mistralai/*`, `Qwen/*`, any HF model | `apiKey` (hf_token) |
| **FreeLLM** ğŸ†“ | `TinyLlama/*`, `Qwen/*`, `microsoft/phi-2` | None (free!) |

### Model Auto-Detection

OneLLM automatically routes to the correct provider based on model name:

```
"gpt-4"           â†’ OpenAI
"claude-3-opus"   â†’ Anthropic
"gemini-1.5-pro"  â†’ Google
"llama-3-70b"     â†’ Groq
"grok-1"          â†’ xAI
```

You can also use explicit provider prefixes:

```
"openai/gpt-4"
"anthropic/claude-3-opus"
"google/gemini-pro"
"azure/my-deployment"
"huggingface/meta-llama/Llama-3.3-70B-Instruct"
"hf/mistralai/Mistral-7B-Instruct-v0.3"
"freellm/TinyLlama/TinyLlama-1.1B-Chat-v1.0"
"free/Qwen/Qwen2.5-0.5B-Instruct"
```

---

## ğŸŒ REST API

### Base URL

```
http://localhost:8080/api
```

### Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/chat/completions` | Synchronous chat completion |
| `POST` | `/chat/completions/stream` | Streaming chat completion (SSE) |
| `GET` | `/providers` | List supported providers |
| `GET` | `/health` | Health check |

---

### `POST /api/chat/completions`

Send a chat completion request with your own API key.

**Request Body:**

```json
{
  "apiKey": "sk-your-api-key-here",
  "model": "gpt-4",
  "messages": [
    { "role": "system", "content": "You are a helpful assistant." },
    { "role": "user", "content": "Hello, who are you?" }
  ],
  "temperature": 0.7,
  "maxTokens": 1000,
  "topP": 0.9,
  "frequencyPenalty": 0.0,
  "presencePenalty": 0.0,
  "stop": ["END"],
  "stream": false,
  
  "baseUrl": "https://custom-endpoint.com/v1",
  "azureResourceName": "my-resource",
  "azureDeploymentName": "gpt-4",
  "openRouterSiteName": "MyApp",
  "openRouterSiteUrl": "https://myapp.com"
}
```

**Request Attributes:**

| Field | Type | Required | Description | Valid Range |
|-------|------|----------|-------------|-------------|
| `apiKey` | `string` | âœ… **Yes** | Your API key for the provider | - |
| `model` | `string` | âœ… **Yes** | Model identifier (routes to provider automatically) | - |
| `messages` | `array` | âœ… **Yes** | Array of message objects | Min 1 message |
| `messages[].role` | `string` | âœ… **Yes** | Role: `system`, `user`, or `assistant` | - |
| `messages[].content` | `string` | âœ… **Yes** | Message content | - |
| `temperature` | `number` | âŒ No | Sampling temperature | `0.0` - `2.0` |
| `maxTokens` | `integer` | âŒ No | Maximum tokens to generate | `â‰¥ 1` |
| `topP` | `number` | âŒ No | Nucleus sampling probability | `0.0` - `1.0` |
| `frequencyPenalty` | `number` | âŒ No | Frequency penalty | `-2.0` - `2.0` |
| `presencePenalty` | `number` | âŒ No | Presence penalty | `-2.0` - `2.0` |
| `stop` | `array` | âŒ No | Stop sequences | - |
| `stream` | `boolean` | âŒ No | Enable streaming | - |

**Provider-Specific Fields:**

| Field | Type | Required For | Description |
|-------|------|--------------|-------------|
| `baseUrl` | `string` | Optional | Custom base URL (OpenAI-compatible endpoints) |
| `azureResourceName` | `string` | Azure | Your Azure resource name |
| `azureDeploymentName` | `string` | Azure | Your Azure deployment name |
| `openRouterSiteName` | `string` | Optional | Your app name (for OpenRouter) |
| `openRouterSiteUrl` | `string` | Optional | Your app URL (for OpenRouter) |

**Response:**

```json
{
  "id": "chatcmpl-abc123",
  "model": "gpt-4-0613",
  "content": "Hello! I'm an AI assistant powered by GPT-4...",
  "finishReason": "stop",
  "provider": "openai",
  "latencyMs": 1234,
  "usage": {
    "promptTokens": 25,
    "completionTokens": 45,
    "totalTokens": 70
  }
}
```

---

### `POST /api/chat/completions/stream`

Stream responses using Server-Sent Events (SSE).

**Request:** Same as `/chat/completions`

**Response:** SSE stream with events:

```
event: chunk
data: {"content": "Hello"}

event: chunk
data: {"content": ", I'm"}

event: complete
data: {"id": "...", "model": "gpt-4", "content": "Hello, I'm...", ...}
```

---

### `GET /api/providers`

List all supported providers.

**Response:**

```json
{
  "providers": ["openai", "anthropic", "google", "azure", "groq", "cerebras", "ollama", "openrouter", "xai", "copilot", "huggingface", "freellm"],
  "count": 12
}
```

---

### `GET /api/health`

Health check endpoint.

**Response:**

```json
{
  "status": "ok",
  "service": "OneLLM"
}
```

---

## ğŸ“ API Examples

### OpenAI (GPT-4)

```bash
curl -X POST http://localhost:8080/api/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "apiKey": "sk-your-openai-key",
    "model": "gpt-4",
    "messages": [
      {"role": "system", "content": "You are a helpful coding assistant."},
      {"role": "user", "content": "Write a Python function to reverse a string."}
    ],
    "temperature": 0.5,
    "maxTokens": 500
  }'
```

### Anthropic (Claude)

```bash
curl -X POST http://localhost:8080/api/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "apiKey": "sk-ant-your-anthropic-key",
    "model": "claude-3-opus",
    "messages": [
      {"role": "user", "content": "Explain quantum computing in simple terms."}
    ],
    "maxTokens": 1000
  }'
```

### Google Gemini

```bash
curl -X POST http://localhost:8080/api/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "apiKey": "AIza-your-google-key",
    "model": "gemini-1.5-pro",
    "messages": [
      {"role": "user", "content": "What is the meaning of life?"}
    ]
  }'
```

### Azure OpenAI

```bash
curl -X POST http://localhost:8080/api/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "apiKey": "your-azure-api-key",
    "model": "azure/my-gpt4-deployment",
    "azureResourceName": "my-azure-resource",
    "azureDeploymentName": "my-gpt4-deployment",
    "messages": [
      {"role": "user", "content": "Hello from Azure!"}
    ]
  }'
```

### Groq (Fast Inference)

```bash
curl -X POST http://localhost:8080/api/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "apiKey": "gsk_your-groq-key",
    "model": "llama-3-70b",
    "messages": [
      {"role": "user", "content": "Write a haiku about coding."}
    ]
  }'
```

### OpenRouter (100+ Models)

```bash
curl -X POST http://localhost:8080/api/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "apiKey": "sk-or-your-openrouter-key",
    "model": "openrouter/anthropic/claude-3-opus",
    "openRouterSiteName": "MyApp",
    "openRouterSiteUrl": "https://myapp.com",
    "messages": [
      {"role": "user", "content": "Hello via OpenRouter!"}
    ]
  }'
```

### xAI (Grok)

```bash
curl -X POST http://localhost:8080/api/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "apiKey": "xai-your-xai-key",
    "model": "grok-1",
    "messages": [
      {"role": "user", "content": "Tell me a joke."}
    ]
  }'
```

### Ollama (Local Models)

```bash
curl -X POST http://localhost:8080/api/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "apiKey": "not-required",
    "model": "ollama/llama2",
    "baseUrl": "http://localhost:11434",
    "messages": [
      {"role": "user", "content": "Hello from local Ollama!"}
    ]
  }'
```

### Hugging Face

```bash
curl -X POST http://localhost:8080/api/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "apiKey": "hf_your-huggingface-token",
    "model": "huggingface/meta-llama/Llama-3.3-70B-Instruct",
    "messages": [
      {"role": "user", "content": "Hello from Hugging Face!"}
    ],
    "maxTokens": 500
  }'
```

### Streaming Example

```bash
curl -X POST http://localhost:8080/api/chat/completions/stream \
  -H "Content-Type: application/json" \
  -d '{
    "apiKey": "sk-your-openai-key",
    "model": "gpt-4",
    "messages": [{"role": "user", "content": "Count from 1 to 10 slowly."}]
  }'
```

### FreeLLM (Free - No API Key!)

```bash
curl -X POST http://localhost:8080/api/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "freellm/TinyLlama/TinyLlama-1.1B-Chat-v1.0",
    "messages": [
      {"role": "user", "content": "Hello! What can you help me with?"}
    ],
    "maxTokens": 256
  }'
```

> **Note**: FreeLLM is completely free - no API key required! It's hosted on Hugging Face Spaces with no rate limiting or billing. Perfect for testing and development.

**Recommended FreeLLM Models:**
| Model | Size | Speed | Quality |
|-------|------|-------|---------|
| `TinyLlama/TinyLlama-1.1B-Chat-v1.0` | 1.1B | âš¡âš¡âš¡ | â­â­ |
| `Qwen/Qwen2.5-0.5B-Instruct` | 0.5B | âš¡âš¡âš¡ | â­â­ |
| `Qwen/Qwen2.5-1.5B-Instruct` | 1.5B | âš¡âš¡ | â­â­â­ |
| `microsoft/phi-2` | 2.7B | âš¡âš¡ | â­â­â­â­ |

### Streaming Example

```bash
curl -X POST http://localhost:8080/api/chat/completions/stream \
  -H "Content-Type: application/json" \
  -d '{
    "apiKey": "sk-your-openai-key",
    "model": "gpt-4",
    "messages": [{"role": "user", "content": "Count from 1 to 10 slowly."}]
  }'
```

---

## ğŸ’» JavaScript/TypeScript Client

```typescript
// Basic request
const response = await fetch('http://localhost:8080/api/chat/completions', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    apiKey: 'sk-your-api-key',
    model: 'gpt-4',
    messages: [
      { role: 'user', content: 'Hello!' }
    ]
  })
});

const data = await response.json();
console.log(data.content);

// Streaming request
const eventSource = new EventSource('http://localhost:8080/api/chat/completions/stream', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    apiKey: 'sk-your-api-key',
    model: 'gpt-4',
    messages: [{ role: 'user', content: 'Write a story' }]
  })
});

eventSource.addEventListener('chunk', (e) => {
  const data = JSON.parse(e.data);
  process.stdout.write(data.content);
});

eventSource.addEventListener('complete', (e) => {
  console.log('\nDone!');
  eventSource.close();
});
```

---

## ğŸ“¦ SDK Usage

Use OneLLM programmatically in your Java application:

### Basic Usage

```java
import io.onellm.OneLLM;
import io.onellm.core.*;

// Build the client with your API keys
OneLLM llm = OneLLM.builder()
    .openai("sk-your-openai-key")
    .anthropic("sk-ant-your-anthropic-key")
    .google("AIza-your-google-key")
    .build();

// Send a completion request
LLMResponse response = llm.complete(
    LLMRequest.builder()
        .model("gpt-4")
        .system("You are a helpful assistant.")
        .user("Explain quantum computing in simple terms.")
        .temperature(0.7)
        .maxTokens(500)
        .build()
);

System.out.println(response.getContent());
System.out.println("Provider: " + response.getProvider());
System.out.println("Latency: " + response.getLatencyMs() + "ms");
```

### Streaming

```java
llm.streamComplete(
    LLMRequest.builder()
        .model("claude-3-opus")
        .user("Write a story about a robot learning to cook.")
        .build(),
    new StreamHandler() {
        @Override
        public void onChunk(String chunk) {
            System.out.print(chunk);
        }
        
        @Override
        public void onComplete(LLMResponse response) {
            System.out.println("\n\nDone! Tokens: " + response.getUsage().getTotalTokens());
        }
        
        @Override
        public void onError(Throwable error) {
            System.err.println("Error: " + error.getMessage());
        }
    }
);
```

### Builder Methods

```java
OneLLM llm = OneLLM.builder()
    .openai("sk-...")                              // OpenAI
    .openai("sk-...", "https://custom-url.com")    // Custom base URL
    .anthropic("sk-ant-...")                       // Anthropic
    .google("AIza...")                             // Google Gemini
    .azure("api-key", "resource", "deployment")    // Azure OpenAI
    .groq("gsk_...")                               // Groq
    .cerebras("cbs-...")                           // Cerebras
    .ollama()                                      // Ollama (localhost)
    .ollama("http://custom-host:11434")            // Ollama (custom)
    .openRouter("or-...")                          // OpenRouter
    .openRouter("or-...", "MySite", "https://...")  // OpenRouter with site
    .xai("xai-...")                                // xAI
    .copilot("token")                              // GitHub Copilot
    .huggingface("hf_...")                         // Hugging Face
    .huggingface("hf_...", "https://endpoint")     // Hugging Face (dedicated endpoint)
    .freellm()                                     // FreeLLM (free, no API key!)
    .freellm("https://custom-freellm")             // FreeLLM (custom host)
    .provider(myCustomProvider)                    // Custom provider
    .build();
```

---

## ğŸ›¡ï¸ Error Handling

OneLLM provides structured error responses:

```json
{
  "error": true,
  "message": "API key is required",
  "timestamp": "2024-12-12T14:30:00Z",
  "type": "validation_error",
  "fields": {
    "apiKey": "API key is required"
  }
}
```

### Error Types

| Type | HTTP Status | Description |
|------|-------------|-------------|
| `validation_error` | 400 | Invalid request parameters (e.g., missing API key) |
| `model_not_found` | 404 | No provider supports the model |
| `provider_not_configured` | 503 | Provider not configured |
| `authentication_error` | 401 | Invalid API key |
| `rate_limit_error` | 429 | Rate limit exceeded |
| `server_error` | 502 | Provider server error |
| `internal_error` | 500 | Unexpected server error |

---

## ğŸ—ï¸ Project Structure

```
onellm/
â”œâ”€â”€ src/main/java/io/onellm/
â”‚   â”œâ”€â”€ OneLLM.java              # SDK entry point
â”‚   â”œâ”€â”€ OneLLMApplication.java   # Spring Boot application
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ LLMConfig.java       # Spring configuration
â”‚   â”œâ”€â”€ controller/
â”‚   â”‚   â””â”€â”€ ChatController.java  # REST API endpoints
â”‚   â”œâ”€â”€ service/
â”‚   â”‚   â””â”€â”€ ProviderFactory.java # Dynamic provider creation
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ LLMProvider.java     # Provider interface
â”‚   â”‚   â”œâ”€â”€ LLMRequest.java      # Request model
â”‚   â”‚   â”œâ”€â”€ LLMResponse.java     # Response model
â”‚   â”‚   â”œâ”€â”€ Message.java         # Chat message
â”‚   â”‚   â”œâ”€â”€ StreamHandler.java   # Streaming callback
â”‚   â”‚   â””â”€â”€ Usage.java           # Token usage
â”‚   â”œâ”€â”€ dto/
â”‚   â”‚   â”œâ”€â”€ ChatCompletionRequest.java
â”‚   â”‚   â”œâ”€â”€ ChatCompletionResponse.java
â”‚   â”‚   â””â”€â”€ MessageDTO.java
â”‚   â”œâ”€â”€ exception/
â”‚   â”‚   â”œâ”€â”€ GlobalExceptionHandler.java
â”‚   â”‚   â”œâ”€â”€ LLMException.java
â”‚   â”‚   â”œâ”€â”€ ModelNotFoundException.java
â”‚   â”‚   â””â”€â”€ ProviderNotConfiguredException.java
â”‚   â”œâ”€â”€ providers/
â”‚   â”‚   â”œâ”€â”€ BaseProvider.java
â”‚   â”‚   â”œâ”€â”€ OpenAIProvider.java
â”‚   â”‚   â”œâ”€â”€ AnthropicProvider.java
â”‚   â”‚   â”œâ”€â”€ GoogleProvider.java
â”‚   â”‚   â”œâ”€â”€ AzureOpenAIProvider.java
â”‚   â”‚   â”œâ”€â”€ GroqProvider.java
â”‚   â”‚   â”œâ”€â”€ CerebrasProvider.java
â”‚   â”‚   â”œâ”€â”€ OllamaProvider.java
â”‚   â”‚   â”œâ”€â”€ OpenRouterProvider.java
â”‚   â”‚   â”œâ”€â”€ XAIProvider.java
â”‚   â”‚   â”œâ”€â”€ CopilotProvider.java
â”‚   â”‚   â”œâ”€â”€ HuggingFaceProvider.java
â”‚   â”‚   â””â”€â”€ FreeLLMProvider.java
â”‚   â””â”€â”€ util/
â”‚       â””â”€â”€ HttpClientWrapper.java
â””â”€â”€ pom.xml
```

---

## ğŸ”’ Security Notes

- **API keys are never stored** on the server
- Each request is processed independently with the provided credentials
- Use HTTPS in production to encrypt API keys in transit
- Consider implementing rate limiting for production deployments

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

Built with â¤ï¸ using:
- [Spring Boot](https://spring.io/projects/spring-boot)
- [Apache HttpClient 5](https://hc.apache.org/httpcomponents-client-5.3.x/)
- [Gson](https://github.com/google/gson)

---

<p align="center">
  Made with â˜• by the OneLLM Team
</p>
